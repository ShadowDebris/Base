# 逐步回归
# 
# 每一步添加或删除一个变量，知道达到某个判停标准为止
# 
# 向前回归：每次添加一个变量进入模型（回归方程），
# 直到添加变量不会使模型有所改进为止；
# 向后回归：首先囊括所有预测变量/解释变量，
# 逐一删除变量，直到再删除变量会降低模型质量为止；
# 
# 向前向后回归（一般简称为逐步回归）：
# 结合向前回归与向后回归，每次选中一个变量加入模型，
# 但是在每一步，也会对模型中现有的变量进行评价，
# 对模型没有贡献的变量会被删除。
# 因此每一步都有可能在删除/添加变量，
# 同一个变量可能会被删除/添加好几次，
# 直到获得最优模型为止
#
# Yuehan Yang. 2014/11/18 yyh@cufe.edu.cn

# 逐步回归函数：
step()
# 或者
# MASS包中的 
stepAIC()
# 均是用AIC准则的逐步回归计算函数
# AIC
# Akaike's Information Criterion 的缩写
# 可以看做某种统计量，以该统计量的最小值来选择模型
# AIC = 2p + n \ln(RSS/n)
# 引入了p值，意味着解释变量过多也会影响模型效果
# 可与OLS进行比较

step(object, scope, scale=0,
     direction=c("both", "backward", "forward"), ...)
     # object是线性模型或广义线性模型分析的结果, 
     # scope是确定逐步搜索的区域, 
     # direction确定逐步搜索的方向: 
     #   “both”是“一切子集回归法”,“backward”是“向后回归”,“forward”是向前回归, 
     # 默认值为both
     # 但若scope缺失，则默认向后回归
     
     
# e.g. 研究目标：某水泥在凝固时释放的热量（y）
# 四个解释变量：水泥中的四种化学成分(X_1, X_2, X_3, X_4)
# 问：选出主要变量，建立 y 与它们的线性回归方程
# 解
# 输入数据，构造数据集
cement<-data.frame(
  X1=c( 7, 1, 11, 11, 7, 11, 3, 1, 2, 21, 1, 11, 10),
  X2=c(26, 29, 56, 31, 52, 55, 71, 31, 54, 47, 40, 66, 68),
  X3=c( 6, 15, 8, 8, 6, 9, 17, 22, 18, 4, 23, 9, 8),
  X4=c(60, 52, 20, 47, 33, 22, 6, 44, 22, 26, 34, 12, 12),
  Y =c(78.5, 74.3, 104.3, 87.6, 95.9, 109.2, 102.7, 72.5,
       93.1,115.9, 83.8, 113.3, 109.4)
)
# 构造数据框的一种方法，常用方法还有
x<-c(274, 180, 375, 205, 86, 265, 98, 330, 195, 53,430, 372, 236, 157, 370)
y<-c(162, 120, 223, 131, 67, 169, 81, 192, 116, 55,
    252, 234, 144, 103, 212)
A<-data.frame(x, y)

# 构造拟合模型
# 不出意外的，用lm()函数
lm.sol<-lm(Y ~ X1+X2+X3+X4, data=cement)

# 下面用函数 step() 作逐步回归
lm.step <- step(lm.sol)

# Start:  AIC=26.94
# Y ~ X1 + X2 + X3 + X4
# 
#         Df Sum of Sq    RSS    AIC
# - X3    1    0.1091   47.973 24.974
# - X4    1    0.2470   48.111 25.011
# - X2    1    2.9725   50.836 25.728
# <none>                47.864 26.944
# - X1    1   25.9509   73.815 30.576
# 
# Step:  AIC=24.97
# Y ~ X1 + X2 + X4
# 
#        Df    Sum of Sq    RSS    AIC
# <none>                 47.97   24.974
# - X4    1      9.93    57.90   25.420
# - X2    1     26.79    74.76   28.742
# - X1    1    820.91   868.88   60.629

结果分析：
1. 使用所有变量构造回归方程
AIC = 26.94 (见74行)

2. 去掉变量X_3，回归方程的AIC值为24.974（71行）
72行去掉X_4,73行去掉X_2,75行去掉X_1,如此类推

3. 去掉X_3获得的AIC最小，进入下一轮计算

4. 下一轮计算中，再删除了X_3后再逐步删除其他变量

5. 结果：81行不删除任何其他变量构造的回归模型AIC最小
终止运算，得到最优回归方程
     
# 对比一下用逐步回归和不用逐步回归的两个回归模型的差异
summary(lm.step)
summary(lm.sol)

# 逐步回归:
#               Estimate Std. Error  t value Pr(>|t|)    
# (Intercept)    71.6483    14.1424 5.066   0.000675 ***
#   X1            1.4519     0.1170  12.410 5.78e-07 ***
#   X2            0.4161     0.1856   2.242 0.051687 .  
#   X4           -0.2365     0.1733  -1.365 0.205395    
# ---

# 不逐步回归
#              Estimate Std. Error t value Pr(>|t|)  
# (Intercept)  62.4054    70.0710   0.891   0.3991  
# X1            1.5511     0.7448   2.083   0.0708 .
# X2            0.5102     0.7238   0.705   0.5009  
# X3            0.1019     0.7547   0.135   0.8959  
# X4           -0.1441     0.7091  -0.203   0.8441  

结果分析：回归系数检验的显著性水平有很大提高，
但 X_2, X_4的系数检验的显著性水平仍然不理想

# 进一步处理
# R 中海油两个函数可以用来作逐步回归：add1(), drop1()

# 使用格式如下：
add1(object, scope, ...)
drop1(object, scope, ...)


# 卡方检验
add1(object, scope, scale=0, test=c("none", "Chisq"),
     k=2, trace=FALSE, ...)
drop1(object, scope, scale=0, test=c("none", "Chisq"),
      k=2, trace=FALSE, ...)

# 增加 F 检验
add1(object, scope, scale=0, test=c("none", "Chisq", "F"),
     x=NULL, k=2, ...)
drop1(object, scope, scale=0, all.cols=TRUE,
      test=c("none", "Chisq", "F"), k=2, ...)

# objcet: 拟合模型
# scope: 模型考虑增加或去掉的项构成的公式
# scale: 计算 C_p 的残差的均方估计（缺省值为 0 或 NULL）

# e.g.
drop1(lm.step)
# Single term deletions
# 
# Model:
#   Y ~ X1 + X2 + X4
#         Df  Sum of Sq    RSS    AIC
# <none>                47.97   24.974
# X1      1    820.91  868.88   60.629
# X2      1     26.79  74.76    28.742
# X4      1      9.93  57.90    25.420

结果分析：
去掉变量X_4，AIC值会从24.97增加到25.42，是增加得最少的

除了AIC以外，残差平方和也是逐步回归的重要指标之一
从直观上来看，拟合越好的方程，残差平方和应该越小

因此，去掉变量X_4，残差平方和上升9.93，也是增加最少的一项

综上分析，认为可以去除变量X_4

lm.opt<-lm(Y ~ X1+X2, data=cement)
summary(lm.opt)
# Coefficients:
#                Estimate   Std. Error t value Pr(>|t|)    
# (Intercept)   52.57735    2.28617   23.00    5.46e-10 ***
#   X1           1.46831    0.12130   12.11    2.69e-07 ***
#   X2           0.66225    0.04585   14.44    5.03e-08 ***
#   ---
这次的所有系数检验的显著性水平都很不错，得到“最优”回归方程：

\hat y = 52.58 + 1.468 X_1+0.6622 X_2
