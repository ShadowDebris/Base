# 一元线性回归分析
# 又称一维线性回归
# 
# 最简单的一对一的因果关系
#
# Yuehan Yang. 2014/11/5 yyh@cufe.edu.cn

# 我们刚刚提到了变量之间的回归分析，
# 即有的是自变量，有的是因变量的情况，
# 对应到一元回归分析中
# 即只考虑一个因变量 y 与一个自变量 X 的关系


# 先看一个例子

# e.g. 
# X: 10 个企业的固定资产
# y: 企业对应的产值
# 数据如下
x <- c(318, 910, 200, 409, 425, 502, 314, 1210, 1022, 1225)
y <- c(524, 1019, 638, 815, 913, 928, 605, 1516, 1219, 1624)

# 首先，描述两变量之间相关关系最常用的图形
plot(x, y)
# 通过图形分析
# 十个点都在一条线附近
abline(380,1)
# abline(a,b,...)
# a 截距
# b 斜率
# 从而判定二者之间拥有某种线性关系：
# y = 380 + X + \epsilon(误差项)

# 对于一般的一元线性回归分析，
# 一般都做出类似的假定
# y = \beta_0 + \beta_1 X + \epsilon_1
# y 响应变量
# X 解释变量
# （在某些环境中，我们也称 X 为预测变量）
# \beta_0 回归常数
# 该数在复杂运算中通常可以通过标准化之间消除，所以 \beta_0 = 0
# \beta_1 回归系数
# 以上二者统称为回归参数
# \epsilon 随机误差
# 一般假设为服从 (\mu, \sigma^2) 的某分部，
# 更一般的，在课堂中一般考虑正态分布即可

# 如前例，对应 X, y, 各自有 10 个样本
# 即 n = 10
# y_i = \beta_0 + \beta_1 X_i + \epsilon_i
# i=1,...,10
# 注
# 上式仍然是一元（一维）线性回归，
# 回归参数不变，不同样本之间的线性差异由随机误差涵盖。

# 现在的问题是
# 如何估计 \beta0, \beta1 ?
# 进一步的，回归方程的显著性检验


